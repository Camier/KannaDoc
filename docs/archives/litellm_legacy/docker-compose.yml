# ============================================================================
# LiteLLM Docker Compose Configuration (Official Best Practices)
# ============================================================================
# WARNING: Requires .env file with POSTGRES_PASSWORD and REDIS_PASSWORD set.
# Copy .env.example to .env and update with secure passwords.
# ============================================================================
services:
  # ==========================================================================
  # LiteLLM Proxy Service
  # ==========================================================================
  litellm:
    # PRODUCTION TIP: Pin to a specific version for stability
    image: ghcr.io/berriai/litellm:v1.81.0-stable
    container_name: litellm-proxy
    restart: unless-stopped
    ports:
      - "4000:4000"
      - "4001:4001"
    # Enable host.docker.internal on Linux for Ollama access
    # Note: Use custom network gateway (172.19.0.1) not host-gateway
    extra_hosts:
      - "host.docker.internal:172.19.0.1"
    env_file:
      - .env
    environment:
      # Use service names for networking (Docker DNS)
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      
      # Production Best Practice: Disable load_dotenv() functionality
      # Ref: https://docs.litellm.ai/docs/proxy/prod
      - LITELLM_MODE=PRODUCTION
      
      # Production Best Practice: Set log level to ERROR (turn off FASTAPI default info logs)
      # Ref: https://docs.litellm.ai/docs/proxy/prod
      - LITELLM_LOG=ERROR
      
      # Production Best Practice: Use separate health check app for reliable K8s probes
      # Prevents false pod restarts during high load
      # Ref: https://docs.litellm.ai/docs/proxy/prod
      - SEPARATE_HEALTH_APP=1
      - SEPARATE_HEALTH_PORT=4001
      
      # Production Best Practice: Graceful shutdown timeout (default: 3600s / 1 hour)
      # Allows in-flight requests to complete before shutdown
      # Ref: https://docs.litellm.ai/docs/proxy/prod
      - SUPERVISORD_STOPWAITSECS=3600
      
      # Production Best Practice: Worker recycling to mitigate memory leaks
      # Ref: https://docs.litellm.ai/docs/proxy/prod
      - MAX_REQUESTS_BEFORE_RESTART=10000
      
      # Production Best Practice: Enable JSON logs for structured logging
      # Ref: https://docs.litellm.ai/docs/proxy/prod
      - JSON_LOGS=true
      
      # Production Best Practice: Use prisma migrate deploy (not db push)
      # Ref: https://docs.litellm.ai/docs/proxy/prod
      - DISABLE_SCHEMA_UPDATE=true
      - USE_PRISMA_MIGRATE=true
    volumes:
      - ./config.yaml:/app/config.yaml:ro
      - ./schema.prisma:/app/schema.prisma:ro
      - ./migrations:/app/migrations:ro
      - ./state/files:/app/state/files
      - ./model_cost_map.local.json:/app/model_cost_map.local.json:ro
      - ./logs:/app/logs
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    # Production Best Practice: Match workers to CPU count for optimal performance
    # Use Gunicorn for stable worker recycling (more mature than Uvicorn)
    # Ref: https://docs.litellm.ai/docs/proxy/prod
    # Note: $(nproc) doesn't work in docker-compose, use 4 workers (adjust based on CPU)
    command: 
      - "--config"
      - "/app/config.yaml"
      - "--port"
      - "4000"
      - "--num_workers"
      - "4"
      - "--run_gunicorn"
      - "--max_requests_before_restart"
      - "10000"
    networks:
      - litellm-net
      - layra-net

  # ==========================================================================
  # PostgreSQL Database Service
  # ==========================================================================
  postgres:
    image: postgres:16-alpine
    container_name: litellm-postgres
    restart: unless-stopped
    ports:
      - "127.0.0.1:5435:5432"
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-litellm}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB:-litellm}
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-litellm} -d ${POSTGRES_DB:-litellm}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - litellm-net

  # ==========================================================================
  # Redis Cache Service
  # ==========================================================================
  redis:
    image: redis:7-alpine
    container_name: litellm-redis
    restart: unless-stopped
    ports:
      - "127.0.0.1:6380:6379"
    command: redis-server --requirepass ${REDIS_PASSWORD} --appendonly yes
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a ${REDIS_PASSWORD} ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - litellm-net

networks:
  litellm-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.19.0.0/16
          gateway: 172.19.0.1
  layra-net:
    external: true
    name: layra_layra-net

volumes:
  postgres-data:
    driver: local
  redis-data:
    driver: local
