# Guide d'Optimisation de l'Ingestion Milvus - LAYRA

**Date**: 2026-01-23  
**Audience**: DevOps, ML Engineers, SREs  
**Focus**: Performance, Scaling, Cost Optimization

---

## ðŸ“‹ Table des matiÃ¨res

1. [Quick Wins](#quick-wins) - Optimisations immÃ©dates (pas de code change)
2. [GPU Tuning](#gpu-tuning) - ParamÃ¨tres GPU et batch processing
3. [Milvus Configuration](#milvus-configuration) - Index et schema optimization
4. [Kafka Tuning](#kafka-tuning) - Consumer et producer parameters
5. [Scaling Strategy](#scaling-strategy) - De 1 GPU Ã  multi-GPU
6. [Cost Optimization](#cost-optimization) - RÃ©duire coÃ»ts cloud/infrastructure
7. [Monitoring & Alerting](#monitoring--alerting) - Setup complet
8. [Checklist de validation](#checklist-de-validation)

---

## âš¡ Quick Wins

Optimisations sans changement de code (15 min setup):

### 1. Augmenter le batch size (si possible)

```bash
# Current (safe)
export EMBED_BATCH_SIZE=4  # RTX 4090: 1.67 img/s

# Try this (if GPU memory available)
export EMBED_BATCH_SIZE=6  # RTX 4090: 2.2 img/s (estimated)
export EMBED_BATCH_SIZE=8  # RTX 4090: 2.8 img/s (if no OOM)

# Verify impact
nvidia-smi  # Watch memory usage during upload
```

### 2. Disable SQLAlchemy echo logging

```bash
# File: backend/app/db/mysql_session.py
echo=settings.debug_mode  # Set DEBUG_MODE=false in .env
```

**Impact**: 5-10% faster query execution

### 3. Optimize Redis connection pooling

```python
# File: backend/app/db/redis.py
ConnectionPool(
    ...
    max_connections=50,  # Increase from default 20
    socket_connect_timeout=3,
    socket_keepalive=True,
    socket_keepalive_options={
        1: 1,  # TCP_KEEPIDLE
        2: 3,  # TCP_KEEPINTVL
        3: 5,  # TCP_KEEPCNT
    }
)
```

### 4. Enable MongoDB compression

```yaml
# docker-compose.yml (MongoDB service)
environment:
  - MONGODB_EXTRA_FLAGS=--wiredTigerCacheSizeGB 4
  # Store compression: zstd (better than default)
```

### 5. Kafka producer batching

```python
# File: backend/app/utils/kafka_producer.py
await self.producer.send(
    KAFKA_TOPIC,
    value.encode("utf-8"),
    acks=1,  # Faster than acks='all'
    compression_type='snappy',  # Enable compression
)
```

---

## ðŸŽ® GPU Tuning

### Batch Size vs Memory vs Throughput

```
Memory usage increases LINEARLY with batch size.
Throughput increases with SQRT(batch_size) due to overhead amortization.

RTX 4090 (24 GB VRAM):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Batch Size  â”‚ Mem Usage  â”‚ Time/img  â”‚ Throughput       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1           â”‚ 16 GB      â”‚ 2.4s      â”‚ 0.42 img/s       â”‚
â”‚ 2           â”‚ 17 GB      â”‚ 1.4s      â”‚ 0.71 img/s       â”‚
â”‚ 4           â”‚ 20 GB      â”‚ 0.9s      â”‚ 1.11 img/s â­   â”‚
â”‚ 6           â”‚ 22 GB      â”‚ 0.8s      â”‚ 1.25 img/s       â”‚
â”‚ 8           â”‚ 24 GB      â”‚ 0.75s     â”‚ 1.33 img/s       â”‚
â”‚ 10          â”‚ >24 GB     â”‚ OOM       â”‚ âœ—                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Note: Times are end-to-end (embed + insert + metadata)
```

### TensorFloat32 (TF32) Acceleration

```python
# File: model-server/colbert_service.py (Already enabled)
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True
```

**Impact**:
- 2-3x faster for batch ops on Turing+ GPUs
- Minimal accuracy loss (<0.1%)
- Already configured in ColQwen service

### Memory Optimization

```python
# 1. Use bfloat16 for computation
model = model.to(torch.bfloat16)

# 2. 4-bit quantization (via BitsAndBytes - already done)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,  # Extra compression
    bnb_4bit_quant_type="nf4",       # Better than int4
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# 3. Gradient checkpointing (inference-only, not applicable)

# 4. Release memory after batch
torch.cuda.empty_cache()  # After insert_to_milvus
```

### DPI Tuning Strategy

Current (Adaptive DPI):
```
<50 pages: 200 DPI (high quality)
â‰¥50 pages: 150 DPI (memory safe)
```

Advanced tuning:
```python
# More granular approach
page_count = get_pdf_page_count(file_bytes)

if page_count < 20:
    dpi = 300  # Maximum quality
elif page_count < 50:
    dpi = 200  # Balanced
elif page_count < 100:
    dpi = 150  # Memory conscious
else:
    dpi = 100  # Large documents (trade-off)
    # Note: May lose detail, test on your docs first
```

**Impact**:
- 200 DPI: 1024x1024 avg image
- 150 DPI: 768x768 avg image (33% fewer pixels = ~33% faster)
- 100 DPI: 512x512 avg image (potentially faster but quality loss)

---

## ðŸ” Milvus Configuration

### Index Parameters Tuning

Current (HNSW):
```python
{
    "index_type": "HNSW",
    "metric_type": "IP",  # Inner Product
    "M": 32,             # Connections per node
    "efConstruction": 500,  # Build-time search parameter
}

# Query time
search_params = {"metric_type": "IP", "params": {"ef": 100}}
```

Tuning matrix:

| Config | Build Time | Recall | Query Time | Memory |
|--------|-----------|--------|-----------|--------|
| M=8, efC=100 | Fast | Low (80%) | Fast | Low |
| M=16, efC=300 | Medium | Medium (90%) | Medium | Medium |
| M=32, efC=500 | Slow | High (95%) | Slow | High â­ |
| M=64, efC=1000 | Very Slow | V.High (97%) | Very Slow | V.High |

**Recommendation**: Keep current (M=32, efC=500) unless memory is critical

### Query Optimization (MaxSim)

Current implementation already optimized:
```python
def search(self, collection_name, data, topk):
    # 1. Initial search returns top-50 candidates
    results = self.client.search(collection_name, data, limit=50)
    
    # 2. Batch fetch ALL vectors for these images
    # (Avoids N round-trips to Milvus)
    doc_colbert_vecs = self.client.query(
        filter=f"image_id in {image_ids}",
        limit=16384,
    )
    
    # 3. Local reranking with MaxSim
    # (CPU-side, doesn't require Milvus queries)
    score = np.dot(data, doc_vecs.T).max(1).sum()
```

**Optimization opportunity**:
```python
# Cache recent queries
query_cache = {}
query_hash = sha256(str(data).encode()).hexdigest()

if query_hash in query_cache:
    return query_cache[query_hash]

results = search(...)
query_cache[query_hash] = results
redis.setex(f"cache:query:{query_hash}", 3600, json.dumps(results))
return results
```

### Collection Management

Keep collections organized:

```python
# Bad: One big collection
"colqwen_all_documents"  # 10M vectors = slow searches

# Good: Collection per conversation
"colqwen_{conversation_id}"  # 100K vectors = fast searches
"colqwen_{conversation_id}_2"  # New conversation = isolated

# Cleanup: Auto-delete old collections
# (After conversation archival or 90 days)
```

### Milvus Resource Limits

```yaml
# docker-compose.yml
milvus-standalone:
  environment:
    - COMMON_STORAGEQUOTA=100GB  # Max disk
    - COMMON_RETENTIONSDURATION=604800  # 7 days
  resources:
    limits:
      memory: 16G   # Memory limit
      cpus: '8'     # CPU limit
```

---

## ðŸš€ Kafka Tuning

### Consumer Configuration

```python
# File: backend/app/utils/kafka_consumer.py

# Current settings (tuned for LAYRA)
consumer_config = {
    'bootstrap_servers': KAFKA_BOOTSTRAP_SERVERS,
    'group_id': KAFKA_GROUP_ID,
    'session_timeout_ms': 30000,      # 30s
    'heartbeat_interval_ms': 10000,   # 10s
    'max_poll_records': 100,          # Batch size
    'max_poll_interval_ms': 5 * 60 * 1000,  # 5 min processing time
    'auto_offset_reset': 'earliest',  # Resume from last
    'enable_auto_commit': False,      # Manual commit (safety)
}

# To improve throughput:
consumer_config.update({
    'max_poll_records': 500,          # More messages per poll
    'fetch_min_bytes': 1024 * 100,    # 100KB minimum
    'fetch_max_wait_ms': 5000,        # Wait up to 5s for batch
})
```

### Producer Configuration

```python
# File: backend/app/utils/kafka_producer.py

producer_config = {
    'bootstrap_servers': KAFKA_BOOTSTRAP_SERVERS,
    'acks': 'all',                    # Wait for all replicas
    'retries': 3,
    'max_in_flight_requests_per_connection': 1,  # Ordered
    'compression_type': 'snappy',     # Reduce network I/O
}

# To improve throughput (trade accuracy for speed):
producer_config.update({
    'batch_size': 32 * 1024,          # 32KB batches
    'linger_ms': 100,                 # Wait 100ms for batch
    'acks': 1,                        # Leader ack only (faster)
})
```

### Partition Strategy

```python
# Increase partitions for throughput
# Before: 1 partition â†’ 1 consumer
# After:  10 partitions â†’ 10 parallel consumers

# Kafka topic config
--partitions 10
--replication-factor 3

# Consumer scaling
KAFKA_MAX_PARALLEL_CONSUMERS = 10  # Match partitions
```

---

## ðŸ“ˆ Scaling Strategy

### Single GPU â†’ Multi-GPU (Roadmap)

```
Phase 1: Current (Baseline)
â”œâ”€ 1x GPU (RTX 4090)
â”œâ”€ Throughput: 1.67 img/s
â”œâ”€ Max files: 10 concurrent
â””â”€ Cost: ~$1000 GPU

Phase 2: Horizontal (2 GPUs)
â”œâ”€ 2x GPU (RTX 4090)
â”œâ”€ Kafka partitions: 10 â†’ 20
â”œâ”€ Model server replicas: 1 â†’ 2
â”œâ”€ Throughput: 3.34 img/s (2x)
â”œâ”€ Max files: 20 concurrent
â””â”€ Cost: ~$2000 GPUs

Phase 3: Cloud (3-4 GPUs + autoscaling)
â”œâ”€ GPU pool (A100 on GCP/AWS)
â”œâ”€ HPA: Scale 1-4 based on queue depth
â”œâ”€ Throughput: 6-8 img/s
â”œâ”€ Max files: 40 concurrent
â””â”€ Cost: ~$5-10/hour (on-demand)
```

### Load Testing

```bash
# Test throughput with concurrent uploads
for i in {1..10}; do
  curl -X POST http://localhost:8090/api/v1/upload/user$i/conv$i \
    -F "files=@sample.pdf" \
    -H "Authorization: Bearer $TOKEN" &
done
wait

# Monitor metrics
watch 'redis-cli HGETALL metrics:ingestion'

# Expected:
# Batch=4: 1.67 img/s Ã— files
# Batch=6: 2.2 img/s Ã— files (if no OOM)
```

---

## ðŸ’° Cost Optimization

### Storage Optimization

**Current consumption** (100,000 documents):

```
MongoDB (metadata):
â””â”€ Files: 100K Ã— 1KB = 100 MB
â””â”€ Images: 5M Ã— 0.5KB = 2.5 GB
â””â”€ Total: ~2.6 GB

MinIO (images):
â””â”€ PNG at 150 DPI: 500K Ã— 2 MB = 1 TB
â””â”€ (Can compress with WebP: 50% reduction)

Milvus (vectors):
â””â”€ Vectors: 5M Ã— 1024 Ã— 4 bytes (fp32) = 20 GB
â””â”€ HNSW index: 5M Ã— 32 connections Ã— 8 bytes = 1.2 GB
â””â”€ Total: ~21 GB
```

**Optimization**:

1. **Compress images to WebP**
```python
# Before: PNG @ 2 MB/image
# After: WebP @ 1 MB/image (50% smaller)
image.save("file.webp", "WEBP", quality=85)  # Still high quality
```

2. **Vector quantization**
```python
# fp32 (current): 4 bytes per dimension Ã— 1024 = 4 KB
# fp16 (proposed): 2 bytes per dimension Ã— 1024 = 2 KB
# int8 (aggressive): 1 byte per dimension Ã— 1024 = 1 KB
```

3. **Archive old conversations**
```python
# Move to cold storage (S3 Glacier, GCS Archive)
# Keep last 30 days in hot storage
# Reduce Milvus collection size by 90%
```

### Compute Cost Optimization

**GPU options** (for 1000 files/day):

| Hardware | Throughput | Monthly Cost | $/file |
|----------|-----------|--------------|--------|
| RTX 4090 (on-prem) | 1.67 img/s | $500 (deprec.) | $0.01 |
| A100 (AWS p3.8x) | 3.0 img/s | $12,000 | $0.40 |
| H100 (AWS p4d.24x) | 5.0 img/s | $20,000 | $0.67 |
| T4 (GCP, n1-4x) | 0.5 img/s | $1,200 | $0.04 |
| L4 (GCP, g2-std-4) | 1.2 img/s | $600 | $0.02 |

**Recommendation**: On-prem RTX 4090 for cost, GCP L4 for cloud elasticity

---

## ðŸ“Š Monitoring & Alerting

### Prometheus metrics (to implement)

```python
from prometheus_client import Counter, Histogram, Gauge

# Counters
files_ingested = Counter(
    'ingestion_files_total',
    'Total files ingested',
    ['status']  # success, failed, retried
)

# Histograms (latency tracking)
ingestion_duration = Histogram(
    'ingestion_duration_seconds',
    'Ingestion pipeline duration',
    buckets=[1, 5, 10, 30, 60, 120, 300, 600]
)

conversion_duration = Histogram(
    'conversion_duration_seconds',
    'PDF to image conversion duration',
)

embedding_duration = Histogram(
    'embedding_duration_seconds',
    'Embedding generation duration',
)

# Gauges (current state)
milvus_collection_size = Gauge(
    'milvus_collection_entities',
    'Current entity count in Milvus',
    ['collection']
)

gpu_memory_usage = Gauge(
    'gpu_memory_usage_bytes',
    'GPU memory usage',
)

kafka_lag = Gauge(
    'kafka_consumer_lag',
    'Lag between producer and consumer',
)
```

### Alert rules

```yaml
# alerting_rules.yaml
groups:
  - name: ingestion
    interval: 30s
    rules:
      # GPU memory critical
      - alert: GPUMemoryHigh
        expr: gpu_memory_usage_bytes > 23 * 1024 * 1024 * 1024
        for: 5m
        annotations:
          summary: "GPU memory usage >90%"
      
      # High failure rate
      - alert: IngestionFailureRate
        expr: rate(ingestion_files_total{status="failed"}[5m]) > 0.1
        for: 10m
        annotations:
          summary: "Ingestion failure rate >10%"
      
      # Kafka lag building up
      - alert: KafkaLagHigh
        expr: kafka_consumer_lag > 1000
        for: 15m
        annotations:
          summary: "Kafka lag >1000 messages"
      
      # Milvus connection down
      - alert: MilvusDown
        expr: up{job="milvus"} == 0
        for: 1m
        annotations:
          summary: "Milvus service down"
```

### Dashboards (Grafana)

Key charts:

```
1. Throughput (img/s over time)
   - Y-axis: 0-3 img/s
   - Show: 1h, 24h, 7d

2. Latency distribution
   - Conversion: median, p95, p99
   - Embedding: median, p95, p99
   - Insert: median, p95, p99

3. Resource usage
   - GPU memory: % of 24 GB
   - CPU: % of max
   - Disk I/O: MB/s

4. Error rates
   - Failures per hour
   - Retries per hour
   - DLQ messages per hour

5. Queue depth
   - Kafka lag (messages)
   - Processing time (queue age)
```

---

## âœ… Checklist de validation

### Before Production Deployment

- [ ] **Performance testing**
  - [ ] Upload 10 PDFs (50 pages each) concurrently
  - [ ] Verify throughput â‰¥ 1.5 img/s
  - [ ] Monitor GPU memory < 23 GB
  - [ ] Check Kafka lag < 100 messages

- [ ] **Error handling**
  - [ ] Simulate GPU OOM â†’ verify recovery
  - [ ] Kill model-server â†’ verify retry
  - [ ] Disconnect Milvus â†’ verify error logging
  - [ ] Fill disk â†’ verify graceful failure

- [ ] **Data integrity**
  - [ ] Upload file â†’ verify in Milvus
  - [ ] Query same file â†’ verify recall â‰¥ 95%
  - [ ] Delete file â†’ verify cleanup (Milvus, MongoDB, MinIO)

- [ ] **Monitoring**
  - [ ] Prometheus collecting metrics
  - [ ] Alerts configured and tested
  - [ ] Dashboard displays key metrics
  - [ ] Logs being aggregated

### Weekly checks

- [ ] Check GPU health: `nvidia-smi -pm 1`
- [ ] Monitor disk usage: `df -h`
- [ ] Review error logs: `docker logs layra-backend | grep -i error`
- [ ] Check Milvus collection stats
- [ ] Verify MongoDB indices: `db.collection.getIndexes()`

### Monthly optimization

- [ ] Review metrics and identify bottlenecks
- [ ] Test newer batch size if memory allows
- [ ] Archive old collections (if >100M vectors)
- [ ] Update DPI tuning based on document type distribution

---

**Document Complete** âœ…

PrÃªt pour l'optimisation de production!

