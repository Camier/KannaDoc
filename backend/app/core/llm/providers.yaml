# LLM Provider Configuration
# 
# DETECTION PRIORITY ORDER (as implemented in ProviderClient.get_provider_for_model):
#
# 1. CLIPROXYAPI PRECEDENCE:
#    If CLIPROXYAPI_BASE_URL is set in the environment, the system first attempts to match 
#    the model name against the 'cliproxyapi' models list using a two-way substring match.
#
# 2. GLM FAMILY SPECIAL RESOLUTION:
#    Models containing 'glm-*' variants are routed to the Z.ai provider ('zai').
#    This repo treats Z.ai as the sole GLM provider SSOT (no open.bigmodel.cn endpoints).
#
# 3. GENERIC ORDERED MATCHING:
#    If no specific resolution occurs, the system iterates through the 'providers' 
#    dictionary below in the order defined. The first provider whose 'models' list 
#    contains the model name as a substring will be selected.
#
# NOTE: Provider order in this file determines selection priority for generic matching!

providers:
  # Official DeepSeek API. Used for deepseek-chat and deepseek-reasoner models.
  deepseek:
    base_url: "https://api.deepseek.com/v1"
    env_key: "DEEPSEEK_API_KEY"
    timeout: 180
    vision: false
    models:
      - deepseek-chat
      - deepseek-reasoner

  # Z.ai API (api.z.ai).
  # Primary provider for GLM-4.x models.
  zai:
    base_url: "https://api.z.ai/api/paas/v4"
    env_key: "ZAI_API_KEY"
    timeout: 180
    vision: true
    models:
      - glm-4.5
      - glm-4.5-air
      - glm-4.6
      - glm-4.7

  # Antigravity Proxy API (via Codex CLI).
  # Proxies OpenAI-compatible /v1 endpoints (chat/completions/models) for a
  # fixed allowlist. The live model list is returned by: GET /v1/models.
  cliproxyapi:
    base_url: ""  # Configured via CLIPROXYAPI_BASE_URL
    env_key: "CLIPROXYAPI_API_KEY"
    timeout: 120
    vision: true
    models:
      # Keep aligned with GET /v1/models from the proxy.
      - claude-opus-4-5-thinking
      - claude-opus-4-6-thinking
      - claude-sonnet-4-5
      - claude-sonnet-4-5-thinking
      - codex-5.3
      - gemini-2.5-flash
      - gemini-2.5-flash-lite
      - gemini-3-flash
      - gemini-3-pro-high
      - gemini-3-pro-image
      - gpt-oss-120b-medium
      - tab_flash_lite_preview

  # Ollama Cloud API. Open-source models hosted on Ollama infrastructure.
  # Synced from: GET https://ollama.com/v1/models (2026-02-07)
  ollama-cloud:
    base_url: "https://ollama.com/v1"
    env_key: "OLLAMA_CLOUD_API_KEY"
    timeout: 120
    vision: true
    models:
      # GPT-OSS
      - gpt-oss:120b
      - gpt-oss:20b
      # Qwen family
      - qwen3-coder:480b
      - qwen3-coder-next
      - qwen3-next:80b
      - qwen3-vl:235b
      - qwen3-vl:235b-instruct
      # DeepSeek (Ollama-hosted, not official API)
      - deepseek-v3.2
      - deepseek-v3.1:671b
      # GLM (Ollama-hosted, not Z.ai API)
      - glm-4.6
      - glm-4.7
      # Kimi
      - kimi-k2:1t
      - kimi-k2.5
      - kimi-k2-thinking
      # Gemini (Ollama-hosted)
      - gemini-3-pro-preview
      - gemini-3-flash-preview
      # Gemma family
      - gemma3:4b
      - gemma3:12b
      - gemma3:27b
      # Mistral family
      - mistral-large-3:675b
      - ministral-3:3b
      - ministral-3:8b
      - ministral-3:14b
      - devstral-2:123b
      - devstral-small-2:24b
       # Nvidia
      - nemotron-3-nano:30b
      # Cogito
      - cogito-2.1:671b
      # Other
      - rnj-1:8b

  # MiniMax API. Used for MiniMax-M2.1 models.
  # Matched via generic ordered matching loop.
  minimax:
    base_url: "https://api.minimax.io/v1"
    env_key: "MINIMAX_API_KEY"
    timeout: 120
    vision: false
    models:
      - MiniMax-M2.1

# Default timeout for providers without explicit timeout (fallback)
default_timeout: 120

vision_patterns:
  - gpt-4o
  - claude-3-opus
  - claude-3-sonnet
  - claude-3.5-sonnet
  - claude-4
  - claude-sonnet-4
  - claude-opus-4
  - gemini-2.5-pro
  - gemini-3-pro
  - gemini-3-flash
  - glm-4v
  - qwen3-vl
  - llama3.2-vision
  - llava
